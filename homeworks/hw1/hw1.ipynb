{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdy1FtrRpGcC"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "## Overview\n",
    "This semester, all homeworks will be conducted through Google Colab notebooks. All code for the homework assignment will be written and run in this notebook. Running in Colab will automatically provide a GPU, but you may also run this notebook locally by following [these instructions](https://research.google.com/colaboratory/local-runtimes.html) if you wish to use your own GPU.\n",
    "\n",
    "You will save images in the notebooks to use and fill out a given LaTeX template which will be submitted to Gradescope, along with your notebook code.\n",
    "\n",
    "## Using Colab\n",
    "On the left-hand side, you can click the different icons to see a Table of Contents of the assignment, as well as local files accessible through the notebook.\n",
    "\n",
    "Make sure to go to **Runtime -> Change runtime type** and select **GPU** as the hardware accelerator. This allows you to use a GPU. Run the cells below to get started on the assignment. Note that a session is open for a maximum of 12 hours, and using too much GPU compute may result in restricted access for a short period of time. Please start the homework early so you have ample time to work.\n",
    "\n",
    "**If you load this notebook by clicking \"Open in Colab\" from github, you will need to save it to your own Google Drive to keep your work.**\n",
    "\n",
    "## General Tips\n",
    "In each homework problem, you will implement autoregressive models and run it on various datasets. Oftentime you will run it on two datasets (dataset 1 and dataset 2). In these cases, the expected outputs for dataset 1 are already provided to help as a sanity check.\n",
    "\n",
    "Feel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n",
    "\n",
    "After you complete the assignment, download all of the images outputted in the results/ folder and upload them to the figure folder in the given latex template.\n",
    "\n",
    "There is a lot of freedom in this homework to design write and design your own models. Hyperparameters are given as a guide to show what worked for us, but feel free to explore and use what you find is best!\n",
    "\n",
    "Run the cells below to download and load up the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUVy2glDtoaR",
    "outputId": "25ae66e6-0a38-4c5d-89ad-91b696af731c"
   },
   "outputs": [],
   "source": [
    "!unzip -qq data/hw1_data.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHWosWrbpO5Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os.path as osp\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from deepul.hw1_helper import (\n",
    "    # Q1\n",
    "    visualize_q1_data,\n",
    "    q1_sample_data_1,\n",
    "    q1_sample_data_2,\n",
    "    q1_save_results,\n",
    "    # Q2\n",
    "    q2a_save_results,\n",
    "    q2b_save_results,\n",
    "    visualize_q2a_data,\n",
    "    visualize_q2b_data,\n",
    "    # Q3\n",
    "    q3ab_save_results,\n",
    "    q3c_save_results,\n",
    "    # Q4\n",
    "    q4a_save_results,\n",
    "    q4b_save_results,\n",
    "    # Q5\n",
    "    load_text_data,\n",
    "    visualize_q5_data,\n",
    "    q5a_save_results,\n",
    "    # Q6\n",
    "    load_colored_mnist_text,\n",
    "    load_pretrain_vqvae,\n",
    "    visualize_q6_data,\n",
    "    q6a_save_results,\n",
    ")\n",
    "from deepul import pytorch_util\n",
    "from deepul import utils\n",
    "\n",
    "GPU_ID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E4CMktzo100"
   },
   "source": [
    "# Question 1: 1D Data\n",
    "\n",
    "In this question, we will train simple generative models on discrete 1D data.\n",
    "\n",
    "Execute the cell below to visualize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ehhv2FZGo4_b",
    "outputId": "9d7fcbbc-5b19-47e8-8f9f-23a2c9ca6744"
   },
   "outputs": [],
   "source": [
    "visualize_q1_data(dset_type=1)\n",
    "visualize_q1_data(dset_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSGTVznZqAR3"
   },
   "source": [
    "## Part (a) Fitting a Histogram\n",
    "\n",
    "Let $\\theta = (\\theta_0, \\dots, \\theta_{d-1}) \\in \\mathbb{R}^{d}$ and define the model $p_\\theta(x) = \\frac{e^{\\theta_x}}{\\sum_{x'}e^{\\theta_{x'}}}$\n",
    "\n",
    "Fit $p_\\theta$ with maximum likelihood via stochastic gradient descent on the training set, using $\\theta$ initialized to zero. Use your favorite version of stochastic gradient descent, and optimize your hyperparameters on a validation set of your choice.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. Plot the model probabilities in a bar graph with $\\{0,\\dots,d-1\\}$ on the x-axis and a real number in $[0,1]$ on the y-axis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg0Jmo1PSaE4"
   },
   "source": [
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = q1_sample_data_1()\n",
    "print(train_data[:10])\n",
    "print(train_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram(pytorch_util.MyModule):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Parameter(torch.zeros(d))\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self.theta\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.theta.device\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch,)\n",
    "\n",
    "        Returns\n",
    "        - logits tensor of shape (batch, d)\n",
    "        \"\"\"\n",
    "        return self.logits.unsqueeze(0).expand(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJNa6dHKpEQU"
   },
   "outputs": [],
   "source": [
    "def q1_a(train_data, test_data, d, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n",
    "  test_data: An (n_test,) numpy array of integers in {0, ..., d-1}\n",
    "  d: The number of possible discrete values for random variable x\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "             used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (d,) of model probabilities\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "\n",
    "  train_data = torch.from_numpy(train_data).to(device)\n",
    "  test_data = torch.from_numpy(test_data).to(device)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  model = Histogram(d)\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "  def evaluate():\n",
    "    with torch.no_grad():\n",
    "      logits = model(test_data)\n",
    "      loss = F.cross_entropy(logits, test_data)\n",
    "    return loss.cpu().item()\n",
    "\n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 100\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch)\n",
    "      loss = F.cross_entropy(logits, train_mbatch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  with torch.no_grad():\n",
    "    probs = F.softmax(model.logits, 0).cpu().numpy()\n",
    "\n",
    "  return np.array(train_losses), np.array(test_losses), probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiGBSP-ESeIj"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q1_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "qjK_KReXsqYa",
    "outputId": "a4c6bd70-96fb-430a-8312-8cfab7169ca4"
   },
   "outputs": [],
   "source": [
    "q1_save_results(1, 'a', q1_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "sJVOUEaaZXcA",
    "outputId": "eaded21d-fc51-4f23-ea1a-ab63f3b829e1"
   },
   "outputs": [],
   "source": [
    "q1_save_results(2, 'a', q1_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiyFXlj0rfcr"
   },
   "source": [
    "## Part (b) Fitting Discretized Mixture of Logistics\n",
    "\n",
    "Let us model $p_\\theta(x)$ as a **discretized** mixture of 4 logistics such that $p_\\theta(x) = \\sum_{i=1}^4 \\pi_i[\\sigma((x+0.5 - \\mu_i)/s_i) - \\sigma((x-0.5-\\mu_i)/s_i)]$\n",
    "\n",
    "For the edge case of when $x = 0$, we replace $x-0.5$ by $-\\infty$, and for $x = 99$, we replace $x+0.5$ by $\\infty$.\n",
    "\n",
    "You may find the [PixelCNN++](https://arxiv.org/abs/1701.05517) helpful for more information on discretized mixture of logistics.\n",
    "\n",
    "**Provide the same set of corresponding deliverables as part (a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4dnQIg_TDx6"
   },
   "source": [
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall derive the log-probability $\\log p_\\theta(x)$. Let's define the following functions:\n",
    "$$\n",
    "\\Delta \\sigma_i(x) = \\begin{cases}\n",
    "\\sigma((x+0.5-\\mu_i)/s_i), & x = 0 \\\\\n",
    "1-\\sigma((x-0.5-\\mu_i)/s_i), & x = 99 \\\\\n",
    "\\sigma((x+0.5-\\mu_i)/s_i) - \\sigma((x-0.5-\\mu_i)/s_i)), & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "We then define a weight vector $w \\in \\mathbb{R}^n$ such that $\\pi_i = \\left. e^{w_i} \\middle/ \\sum_j e^{w_j} \\right.$. Then, the log-probability is\n",
    "\\begin{align*}\n",
    "\\log p_\\theta(x) &= \\log \\dfrac{\\displaystyle \\sum_{i=1}^n e^{w_i + \\log \\Delta \\sigma_i(x)} } {\\displaystyle \\sum_{i=1}^n e^{w_i}} \\\\\n",
    "&= \\log \\sum_{i=1}^n e^{w_i + \\log \\Delta \\sigma_i(x)} - \\log \\sum_{i=1}^n e^{w_i} .\n",
    "\\end{align*}\n",
    "We recognize this as the difference of two log-sum-exp functions.\n",
    "\n",
    "We shall now show that $\\log \\Delta \\sigma_i(x)$ has the following form:\n",
    "$$\n",
    "\\log \\Delta \\sigma_i(x) = \\begin{cases}\n",
    "\\log \\sigma((x+0.5-\\mu_i) / s_i), & x = 0 \\\\\n",
    "\\log \\sigma (-(x-0.5-\\mu_i) / s_i), & x = 99 \\\\\n",
    "-(x+0.5-\\mu_i)/s_i - (\\log\\sigma)^{-1}(-1/s_i) + \\log\\sigma((x+0.5-\\mu_i)/s_i) + \\log\\sigma((x-0.5-\\mu_i)/s_i), & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "where $(\\log\\sigma)^{-1}(x) = -\\log(e^{-x}-1)$ (*inverse log-sigmoid function*).\n",
    "\n",
    "For compactness, we define $\\Delta x_i = x - \\mu_i$. For $x = 0$, we have:\n",
    "$$\n",
    "\\log \\Delta\\sigma_i(x) = \\log \\sigma((\\Delta x_i+0.5)/s_i).\n",
    "$$\n",
    "\n",
    "Similarly, for $x = 99$, we have:\n",
    "$$\n",
    "\\log \\Delta\\sigma_i(x) = \\log (1 - \\sigma((\\Delta x_i-0.5)/s_i)) = \\log \\sigma(-(\\Delta x_i-0.5)/s_i)),\n",
    "$$\n",
    "where we take advantage of the fact that $1 - \\sigma(x) = \\sigma(-x)$.\n",
    "\n",
    "Finally, for $0 < x < 99$, we have:\n",
    "\\begin{align*}\n",
    "\\log \\Delta\\sigma_i(x) &= \\log\\left( \\frac{1}{1 + e^{-(\\Delta x_i + 0.5)/s_i}} - \\frac{1}{1 + e^{-(\\Delta x_i - 0.5)/s_i}} \\right) \\\\\n",
    "&= \\log \\frac{ e^{-(\\Delta x_i - 0.5)/s_i} - e^{-(\\Delta x_i + 0.5)/s_i} }{ (1 + e^{-(\\Delta x_i + 0.5)/s_i}) (1 + e^{-(\\Delta x_i - 0.5)/s_i}) } \\\\\n",
    "&= -\\frac{\\Delta x_i + 0.5}{s_i} + \\log(e^{1/s_i} - 1) + \\log\\sigma((\\Delta x_i+0.5)/s_i) + \\log\\sigma((\\Delta x_i-0.5)/s_i) \\\\\n",
    "&= -\\frac{\\Delta x_i + 0.5}{s_i} - (\\log\\sigma)^{-1}(-1/s_i) + \\log\\sigma((\\Delta x_i+0.5)/s_i) + \\log\\sigma((\\Delta x_i-0.5)/s_i) .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsigmoid_inverse(x: torch.Tensor):\n",
    "    return -torch.log(torch.expm1(-x))\n",
    "\n",
    "\n",
    "class DML(pytorch_util.MyModule):\n",
    "    def __init__(self, n_mix: int, d: int):\n",
    "        super().__init__()\n",
    "        self.n_mix = n_mix\n",
    "        self.d = d\n",
    "        self.log_weights = nn.Parameter(torch.zeros(self.n_mix))\n",
    "        self.locs = nn.Parameter(torch.arange(self.n_mix).float() / (self.n_mix - 1) * self.d)\n",
    "        self.log_inv_scales = nn.Parameter(torch.randn(self.n_mix))\n",
    "        self.print_n_params()\n",
    "\n",
    "    def log_discretized_logistic(self, x: torch.Tensor):\n",
    "        x_unsq = x.unsqueeze(1) # [d, 1]\n",
    "        x_shifted = x_unsq.float() - self.locs # [d, n_mix]\n",
    "        scales_inv = torch.exp(self.log_inv_scales)  # [n_mix]\n",
    "        left = F.logsigmoid(scales_inv * (x_shifted+0.5))\n",
    "        right = F.logsigmoid(-scales_inv * (x_shifted-0.5))\n",
    "        middle = (-scales_inv*(x_shifted+0.5) - logsigmoid_inverse(-scales_inv)\n",
    "                  + F.logsigmoid(scales_inv * (x_shifted+0.5)) + F.logsigmoid(scales_inv * (x_shifted-0.5)))\n",
    "        return torch.where(x_unsq == 0, left, torch.where(x_unsq == self.d-1, right, middle))\n",
    "\n",
    "    @property\n",
    "    def logits(self) -> torch.Tensor:\n",
    "        x = torch.arange(self.d, device=self.log_weights.device)  # [d]\n",
    "        log_disc_logistic = self.log_discretized_logistic(x)  # [d, n_mix]\n",
    "        logits_th = torch.logsumexp(self.log_weights + log_disc_logistic, 1) - torch.logsumexp(self.log_weights, 0)\n",
    "        return logits_th\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.log_weights.device\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch,)\n",
    "\n",
    "        Returns\n",
    "        - logits tensor of shape (batch, d)\n",
    "        \"\"\"\n",
    "        return self.logits.unsqueeze(0).expand(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAvMQDJJrjNo"
   },
   "outputs": [],
   "source": [
    "def q1_b(train_data, test_data, d, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n",
    "  test_data: An (n_test,) numpy array of integers in {0, ..., d-1}\n",
    "  d: The number of possible discrete values for random variable x\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (d,) of model probabilities\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "    \n",
    "  train_data = torch.from_numpy(train_data).to(device)\n",
    "  test_data = torch.from_numpy(test_data).to(device)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  n_mix = 4\n",
    "  model = DML(n_mix, d)\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=3e-2)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "  def evaluate():\n",
    "    with torch.no_grad():\n",
    "      logits = model(test_data)\n",
    "      loss = F.cross_entropy(logits, test_data)\n",
    "      return loss.cpu().item()\n",
    "\n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 30 if dset_id == 1 else 10\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch)\n",
    "      loss = F.cross_entropy(logits, train_mbatch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "    \n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  with torch.no_grad():\n",
    "    probs = F.softmax(model.logits, 0).cpu().numpy()\n",
    "  return train_losses, test_losses, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwZyhlewTHH4"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q1_b`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "wnnQORaG6Ouf",
    "outputId": "3890f9ce-84cb-4c48-f2ca-9a1746acc424"
   },
   "outputs": [],
   "source": [
    "q1_save_results(1, 'b', q1_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "1jLGoDa46RM6",
    "outputId": "000a56aa-4811-44ef-a8bb-a2e672cbc106"
   },
   "outputs": [],
   "source": [
    "q1_save_results(2, 'b', q1_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP8lmmk7Xrct"
   },
   "source": [
    "# Question 2 PixelCNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wnyhDNqcAcw"
   },
   "source": [
    "Now, you will train more powerful PixelCNN models on the shapes dataset and MNIST. In addition, we will extend to modeling colored datasets.\n",
    "\n",
    "Run the cell below to visualize the two datasets binary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_q2a_data(1)\n",
    "visualize_q2a_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = utils.load_pickled_data(\"data/shapes.pkl\")\n",
    "print(train_data.dtype)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (a) PixelCNN on Shapes and MNIST\n",
    "In this part, implement a simple PixelCNN architecture to model binary MNIST and shapes images (same as Q2(b), but with a PixelCNN).\n",
    "\n",
    "We recommend the following network design:\n",
    "* A $7 \\times 7$ masked type A convolution\n",
    "* $5$ $7 \\times 7$ masked type B convolutions\n",
    "* $2$ $1 \\times 1$ masked type B convolutions\n",
    "* Appropriate ReLU nonlinearities in-between\n",
    "* 64 convolutional filters\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size 128\n",
    "* Learning rate $10^{-3}$\n",
    "* 10 epochs\n",
    "* Adam Optimizer (this applies to all PixelCNN models trained in future parts)\n",
    "\n",
    "Your model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine). It may also help to scale your input to $[-1, 1]$ before running it through the network. \n",
    "\n",
    "Training on the shapes dataset should be quick, and MNIST should take around 10 minutes\n",
    "\n",
    "Checkout the Paper for more details: https://arxiv.org/abs/1601.06759\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EleefdNuciyc"
   },
   "source": [
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert mask_type in {\"A\", \"B\"}\n",
    "        self.register_buffer(\"mask\", self.weight.data.clone())\n",
    "        _, _, h, w = self.weight.size()\n",
    "        self.mask.fill_(0.)\n",
    "        self.mask[:, :, h // 2, :w // 2 + (mask_type == \"B\")] = 1.\n",
    "        self.mask[:, :, :h // 2, :] = 1.\n",
    "        self._mask_weight()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self._mask_weight()\n",
    "        return super().forward(x)\n",
    "\n",
    "    def _mask_weight(self):\n",
    "        self.weight.data.mul_(self.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the masked convolution implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_a = MaskedConv2d(\"A\", 1, 1, 7, padding=(7-1) // 2)\n",
    "conv_b = MaskedConv2d(\"B\", 1, 1, 7, padding=(7-1) // 2)\n",
    "\n",
    "print(\"Mask A\")\n",
    "print(conv_a.mask.squeeze())\n",
    "print(conv_a.weight.data.squeeze())\n",
    "\n",
    "print()\n",
    "print(\"===========\")\n",
    "print()\n",
    "\n",
    "print(\"Mask B\")\n",
    "print(conv_b.mask.squeeze())\n",
    "print(conv_b.weight.data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryPixelCNN(pytorch_util.MyModule):    \n",
    "    def __init__(self, n_filters: int):\n",
    "        super().__init__()        \n",
    "        self.convs = nn.ModuleList(\n",
    "              [MaskedConv2d(\"A\", 1,         n_filters, 7, padding=(7-1) // 2)]\n",
    "            + [MaskedConv2d(\"B\", n_filters, n_filters, 7, padding=(7-1) // 2) for _ in range(5)]\n",
    "            + [MaskedConv2d(\"B\", n_filters, n_filters, 1)]\n",
    "        )\n",
    "        self.conv_out = MaskedConv2d(\"B\", n_filters, 1, 1)\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.convs[0].weight.device # self.conv_a.weight.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2*x.permute(0, 3, 1, 2).float() - 1\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x))\n",
    "        return self.conv_out(x).permute(0, 2, 3, 1)\n",
    "        \n",
    "\n",
    "    def sample(self, size, n_samples):\n",
    "        with torch.no_grad():\n",
    "            samples = torch.zeros(n_samples, *size, 1, device=self.device, dtype=torch.uint8)\n",
    "            for i in range(size[0]):\n",
    "                for j in range(size[1]):\n",
    "                    logits = self(samples)[:, i, j, 0]\n",
    "                    sampled_pixels = torch.bernoulli(torch.sigmoid(logits)).type(torch.uint8)\n",
    "                    samples[:, i, j, 0] = sampled_pixels\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_a(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "  test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "  image_shape: (H, W), height and width of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "    \n",
    "  train_data = torch.from_numpy(train_data).to(device)\n",
    "  test_data = torch.from_numpy(test_data).to(device)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  model = BinaryPixelCNN(64)\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "  def evaluate():\n",
    "    with torch.no_grad():\n",
    "      logits = model(test_data)\n",
    "      loss = F.binary_cross_entropy_with_logits(logits, test_data.float())\n",
    "      return loss.cpu().item()\n",
    "\n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 15 if dset_id == 1 else 5\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      train_mbatch = train_mbatch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch)\n",
    "      loss = F.binary_cross_entropy_with_logits(logits, train_mbatch.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  return train_losses, test_losses, model.sample(image_shape, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0EPVfz1cpq0"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "xNxXqVZpAd_V",
    "outputId": "e2b74bd6-fde3-41f3-9e3a-e24a8cdee92e"
   },
   "outputs": [],
   "source": [
    "q2a_save_results(1, q2_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "OCyQzhJdAfiJ",
    "outputId": "1bf0668e-43b8-457c-db3b-7ea970af4777"
   },
   "outputs": [],
   "source": [
    "q2a_save_results(2, q2_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J7qlqlODNgL"
   },
   "source": [
    "## Part (b) PixelCNN on Colored Shapes and MNIST: Independent Color Channels\n",
    "\n",
    "For the next part, we'll work with color images (shapes and MNIST). Run the cell below to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "80f_7uZWkDSv",
    "outputId": "61dde7b4-00aa-49e0-bbd3-96bd19f8f8bf"
   },
   "outputs": [],
   "source": [
    "visualize_q2b_data(1)\n",
    "visualize_q2b_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = utils.load_pickled_data(\"data/shapes_colored.pkl\")\n",
    "print(train_data.dtype)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data_one_hot = F.one_hot(torch.from_numpy(train_data).long(), num_classes=4)\n",
    "print(train_data_one_hot.shape)\n",
    "\n",
    "print(train_data[0, :2, :3].squeeze())\n",
    "print(\"=======\")\n",
    "print(train_data_one_hot[0, :2, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y6NggR6gmU9"
   },
   "source": [
    "Now, implement a PixelCNN to support RGB color channels (or augment your existing implementation). **First, implement a PixelCNN that assumes color channels as independent.** More formally, we model the following parameterized distribution:\n",
    "\n",
    "$$p_\\theta(x) = \\prod_{i=1}^{HW}\\prod_{c=1}^C p_\\theta(x_i^c | x_{<i})$$\n",
    "\n",
    "Here are some tips that you may find useful for designing and training these models:\n",
    "* You will need a 4-way softmax for every prediction, as opposed to a 256-way softmax in the PixelCNN paper, since the dataset is quantized to two bits per color channel\n",
    "* You can set the number of filters for each convolutions to 120. You can use the ReLU nonlinearity throughout.\n",
    "* Use a stack of 8 residual block architecture from [Figure 5](https://arxiv.org/abs/1601.06759) but with 7 x 7 masked convolutions in the middle instead of 3 x 3 masked convolutions\n",
    "* Consider using [layer normalization](https://arxiv.org/abs/1607.06450) to improve performance. However, be careful to maintain the autoregressive property.\n",
    "* With a learning rate of $10^{-3}$ and a batch size of 128, it should take a few minutes to run on the shapes dataset, and about 50-60 minutes on MNIST.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwJQG9i1iQOa"
   },
   "source": [
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseColorPixelCNN(pytorch_util.MyModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_colors = 4\n",
    "        self.n_channels = 3\n",
    "\n",
    "    def sample(self, size: Tuple[int, int, int], n_samples: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        size: tuple corresponding to (H, W, C)\n",
    "        n_samples: number of samples\n",
    "\n",
    "        Returns\n",
    "        - a numpy array of size (n_samples, H, W, C) of samples with values in {0, 1, 2, 3}\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            samples = torch.zeros(n_samples, *size, device=self.device, dtype=torch.uint8)\n",
    "            for i in range(size[0]):\n",
    "                for j in range(size[1]):\n",
    "                    logits = self(samples)[:, i, j] # (n_samples, C, colors)\n",
    "                    probs = F.softmax(logits, -1) # (n_samples, C, colors)\n",
    "                    probs_flat = probs.view(-1, self.n_colors) # (n_samples * C, colors)\n",
    "                    sampled_pixels_flat = torch.multinomial(probs_flat, num_samples=1) # (n_samples * C, 1)\n",
    "                    sampled_pixels = sampled_pixels_flat.view(logits.shape[:2]) # (n_samples, C)\n",
    "                    samples[:, i, j] = sampled_pixels\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we implement a residual block, which alternates between layer normalization (optional), ReLU non-linearity, and masked type B convolution.\n",
    "\n",
    "To maintain the autoregressive property, we compute the layer normalization over the *channels* of the feature $x_{b,i,j,c}$, i.e., the mean and variance are computed as:\n",
    "\\begin{align*}\n",
    "\\mu_{b,i,j} &= \\mathbb{E}_{c, (i', j') \\in W(i, j)} [x_{b,i',j',c}] \\\\\n",
    "\\sigma_{b,i,j}^2 &= \\mathrm{var}_{c, (i',j') \\in W(i,j)} (x_{b,i',j',c})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveLayerNorm(nn.Module):\n",
    "    def __init__(self, channels: int, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(channels))\n",
    "        self.register_buffer(\"mean_filter\", torch.zeros(1, channels, kernel_size, kernel_size))\n",
    "        self.mean_filter[:, :, kernel_size // 2, :kernel_size // 2 + 1] = 1.\n",
    "        self.mean_filter[:, :, :kernel_size // 2, :] = 1.\n",
    "        self.mean_filter /= self.mean_filter.sum()\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: a (batch, channels, H, W) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, channels, H, W) float tensor\n",
    "        \"\"\"\n",
    "        means = F.conv2d(x, self.mean_filter, padding=(self.kernel_size-1) // 2)  # (batch, 1, h, w)\n",
    "\n",
    "        deltas = x - means # (batch, channels, H, W)\n",
    "        sq_deltas = torch.square(x - means) # (batch, channels, H,  W)\n",
    "        vars = F.conv2d(sq_deltas, self.mean_filter, padding=(self.kernel_size-1) // 2)  # (batch, 1, H, W)\n",
    "\n",
    "        x_whitened = deltas / torch.sqrt(vars + 1e-5)  # (batch, channels, H, W)\n",
    "\n",
    "        return self.weight[:, None, None] * x_whitened + self.bias[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_filters: int, layer_norm: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            MaskedConv2d(\"B\", n_filters, n_filters, 1),\n",
    "            MaskedConv2d(\"B\", n_filters, n_filters, 7, padding=(7-1) // 2),\n",
    "            MaskedConv2d(\"B\", n_filters, n_filters, 1)\n",
    "        ])\n",
    "\n",
    "        #layer_norm_ctor = nn.LayerNorm if layer_norm else nn.Identity\n",
    "        layer_norm_ctor = AutoregressiveLayerNorm if layer_norm else nn.Identity\n",
    "        self.lns = nn.ModuleList([layer_norm_ctor(n_filters) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, n_filters, H, W) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, n_filters, H, W) float tensor\n",
    "        \"\"\"\n",
    "        y = x\n",
    "\n",
    "        for conv, ln in zip(self.convs, self.lns):\n",
    "            #y = ln(y.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "            y = ln(y)\n",
    "            y = F.relu(y)\n",
    "            y = conv(y)\n",
    "\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first implement a `FullySeparatedColorPixelCNN` which applies which applies 3 `ChannelPixelCNN`'s to the image, one per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelPixelCNN(nn.Module):\n",
    "    def __init__(self, n_filters: int, layer_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_channels = 3\n",
    "        self.n_colors = 4\n",
    "        self.conv_a = MaskedConv2d(\"A\", self.n_channels, n_filters, 7, padding=(7-1) // 2)\n",
    "        self.resids = nn.ModuleList([ResidualBlock(n_filters, layer_norm) for _ in range(8)])\n",
    "        self.conv_proj = MaskedConv2d(\"B\", n_filters, self.n_colors, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, H, W, C) uint8 tensor of color images with values in {0, 1, 2, 3}\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, H, W, 4)\n",
    "        \"\"\"\n",
    "        x = 2/3*x.permute(0, 3, 1, 2).float() - 1 # (batch, C, H, W)\n",
    "        x = self.conv_a(x)\n",
    "        for resid in self.resids:\n",
    "            x = resid(x)\n",
    "        return self.conv_proj(x).permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "class FullySeparatedColorPixelCNN(BaseColorPixelCNN):\n",
    "    def __init__(self, n_filters: int, layer_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.cnns = nn.ModuleList([ChannelPixelCNN(n_filters, layer_norm) for _ in range(self.n_channels)])\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.cnns[0].conv_a.weight.device\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: A (batch, H, W, C) uint8 tensor of color images with values in {0, 1, 2, 3}\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, H, W, C, 4)\n",
    "        \"\"\"\n",
    "        logits = torch.empty(*x.shape, self.n_colors, device=x.device) # (batch, H, W, C, colors)\n",
    "        for i, cnn in enumerate(self.cnns):\n",
    "            logits[:, :, :, i] = cnn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a `MultiTailedColorPixelCNN`, which applies 3 separate modules consisting of masked type A convolution and two residual blocks. The outputted features are then concatenated along the batch dimension and then processed through six residual blocks to form the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTailedColorPixelCNN(BaseColorPixelCNN):\n",
    "    def __init__(self, n_filters: int, layer_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.convs_a = nn.ModuleList([MaskedConv2d(\"A\", self.n_channels, n_filters, 7, padding=(7-1) // 2) for _ in range(self.n_channels)])\n",
    "        self.resids_separate = nn.ModuleList([\n",
    "            nn.ModuleList([ResidualBlock(n_filters, layer_norm) for _ in range(2)])\n",
    "        for _ in range(self.n_channels)])\n",
    "        self.resids_shared = nn.ModuleList([ResidualBlock(n_filters, layer_norm) for _ in range(6)])\n",
    "        self.conv_out = MaskedConv2d(\"B\", n_filters, self.n_colors, 1)\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.conv_out.weight.device\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, H, W, C) uint8 tensor of color images with values in {0, 1, 2, 3}\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, H, W, C, 4)\n",
    "        \"\"\"\n",
    "        x = 2/3*x.permute(0, 3, 1, 2).float() - 1. # (batch, C, H, W)\n",
    "\n",
    "        xs = []\n",
    "        for conv_a, resids in zip(self.convs_a, self.resids_separate):\n",
    "            y = x\n",
    "            y = conv_a(y)\n",
    "            for resid in resids:\n",
    "                y = resid(y)\n",
    "            xs.append(y)\n",
    "\n",
    "        x = torch.cat(xs, dim=0) # (C * batch, n_filters, H, W)\n",
    "        for resid in self.resids_shared:\n",
    "            x = resid(x)\n",
    "        x = self.conv_out(x) # (C * batch, colors, H, W)\n",
    "        x = x.view(self.n_channels, -1, self.n_colors, x.shape[-2], x.shape[-1]) # (C, batch, colors, H, W)\n",
    "        x = x.permute(1, 3, 4, 0, 2) # (batch, H, W, C, 4)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement a `MultiHeadedColorPixelCNN`, which first applies six layers of residual blocks to the image. Then, the outputted features are fed through 3 separate modules (one per channel) of two residual blocks to form the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedColorPixelCNN(BaseColorPixelCNN):\n",
    "    def __init__(self, n_filters: int, layer_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.conv_a = MaskedConv2d(\"A\", self.n_channels, n_filters, 7, padding=(7-1) // 2)\n",
    "        self.resids_shared = nn.ModuleList([ResidualBlock(n_filters, layer_norm) for _ in range(6)])\n",
    "        self.resids_separate = nn.ModuleList([\n",
    "            nn.ModuleList([ResidualBlock(n_filters, layer_norm) for _ in range(2)])\n",
    "            for _ in range(self.n_channels)])\n",
    "        self.convs_out = nn.ModuleList([MaskedConv2d(\"B\", n_filters, self.n_colors, 1) for _ in range(self.n_channels)])\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.conv_a.weight.device\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, H, W, C) uint8 tensor of color images with values in {0, 1, 2, 3}\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, H, W, C, 4)\n",
    "        \"\"\"\n",
    "        x = 2/3*x.permute(0, 3, 1, 2).float() - 1 # (batch, C, H, W)\n",
    "        \n",
    "        x = self.conv_a(x)\n",
    "        for resid in self.resids_shared:\n",
    "            x = resid(x)\n",
    "\n",
    "        xs = []\n",
    "        for resids, conv in zip(self.resids_separate, self.convs_out):\n",
    "            y = x\n",
    "            for resid in resids:\n",
    "                y = resid(y)\n",
    "            y = conv(y)\n",
    "            xs.append(y)\n",
    "\n",
    "        x = torch.stack(xs, dim=-1) # (batch, colors, H, W, C)\n",
    "        x = x.permute(0, 2, 3, 4, 1) # (batch, H, W, C, colors)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NE99xTPJDLM7"
   },
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: None # placeholder\n",
    "\n",
    "def q2_b(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "    \n",
    "  train_data = torch.from_numpy(train_data)\n",
    "  test_data = torch.from_numpy(test_data)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  model = make_pixel_cnn()\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "  def evaluate():\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "      for test_mbatch in test_dataloader:\n",
    "        test_mbatch = test_mbatch.to(device)\n",
    "        logits = model(test_mbatch)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), test_mbatch.ravel())\n",
    "        losses.append(loss.cpu().item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 8 if dset_id == 1 else 3\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      train_mbatch = train_mbatch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch)\n",
    "      loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), train_mbatch.ravel())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  return train_losses, test_losses, model.sample(image_shape, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_b`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully separated PixelCNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try without layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: FullySeparatedColorPixelCNN(90, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "print(\"Fully separated, no layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "print(\"Fully separated, no layer norm\")\n",
    "q2b_save_results(2, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try with layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: FullySeparatedColorPixelCNN(90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fully separated, layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fully separated, layer norm\")\n",
    "q2b_save_results(2, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-tailed PixelCNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try without layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: MultiTailedColorPixelCNN(128, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-tailed, no layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-tailed, no layer norm\")\n",
    "q2b_save_results(2, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try with layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: MultiTailedColorPixelCNN(128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-tailed, layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-tailed, layer norm\")\n",
    "#q2b_save_results(2, 'b', q2_b)\n",
    "#with torch.no_grad():\n",
    "#    torch.cuda.empty_cache()\n",
    "print(\"Fails to run due to insufficient VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-headed PixelCNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try without layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: MultiHeadedColorPixelCNN(128, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-headed, no layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-headed, no layer norm\")\n",
    "q2b_save_results(2, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try with layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pixel_cnn = lambda: MultiHeadedColorPixelCNN(128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-headed, layer norm\")\n",
    "q2b_save_results(1, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-headed, layer norm\")\n",
    "q2b_save_results(2, 'b', q2_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZLcaHwLNNL"
   },
   "source": [
    "# Question 3: Causal Transformer - iGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move onto the current most popular and widespread autoregressive model, the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (a) Autoregressive Transformer on Shapes and MNIST\n",
    "In this part, implement a simple Autoregressive Transformer to model binary MNIST and shapes images (same as Q2(a), but with a Transformer). \n",
    "\n",
    "Some additional notes about your transformer implementation:\n",
    " * iGPT uses learned positional encodings. We recommend to use those here as well. However, you may also use sinusoidal positional encodings if you wish (see the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper)\n",
    " * Autoregressive transformer always predicts the **next** token, give prior tokens. iGPT has a special **\\<bos\\>** or beginning of sequence token at the start of every sequence every image. Make sure to include this in your implementation as well. You can generate unconditional sample by conditioning with the **\\<bos\\>** token.\n",
    " * While dropout is a common feature in transformer models, you do not need to add it (but may if you wish!).\n",
    " * Prebuilt transformers exist in some frameworks (i.e. PyTorch). Don't just use an off the shelf implementation as the point of the exercise is to better understand the transformer architecture. Building the transformer from the ground up (use primitives such as Linear/Dense layers, LayerNorm, GeLU, Embedding)\n",
    " * Learning rate warmup and cosine learning rate decay are often used when training transformers to improve training stability and improve performance. See if this helps your model! Try 1000 steps of warmup with a cosine learning rate decay.\n",
    "\n",
    "Paper references\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) \n",
    "* [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "We recommend the following network design parameters:\n",
    "* $d_{model}$: 128\n",
    "* heads: 4\n",
    "* layers: 2\n",
    "* GeLU nonlinearities\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size: 64 or 32 or 16 (whichever fits in your GPU)\n",
    "* Learning rate: $10^{-3}$\n",
    "* 15 epochs or more\n",
    "* Adam Optimizer (this applies to all Transformers models trained in future parts)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the causal self-attention layer bit by bit, first. We start by defining some hyperparameters and an arbitrary input $X \\in \\mathbb{R}^{L \\times dh}$, where $L$ is the sequence length, $d$ is the embedding dimension, and $h$ is the number of attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "seq_length = 3\n",
    "num_heads = 4\n",
    "embed_dim = 5\n",
    "x = torch.randn(batch, seq_length, embed_dim * num_heads)\n",
    "print(\"X shape:\", x.shape)\n",
    "print(\"X\")\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a layer that maps $X$ to $Q$, $K$, and $V$, each of which is an $L \\times d$ matrix. Namely, we rely on the linear property to efficiently compute them:\n",
    "$$\n",
    "\\begin{bmatrix} Q \\\\ K \\\\ V \\end{bmatrix} = \\begin{bmatrix} W_Q \\\\ W_K \\\\ W_V \\end{bmatrix} X,\n",
    "$$\n",
    "where each weight matrix is $L \\times L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(embed_dim*num_heads, 3*embed_dim*num_heads, bias=False)\n",
    "qkv = qkv_layer(x)\n",
    "print(\"QKV shape:\", list(qkv.size()))  # (batch, seq_length, 3*embed_dim*num_heads)\n",
    "\n",
    "q, k, v = torch.split(qkv, embed_dim*num_heads, dim=2)  # (batch, seq_length, embed_dim*num_heads)\n",
    "print(\"V shape:\", list(v.size()))\n",
    "print(\"V:\")\n",
    "print(v.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing multi-head attention, we evenly partition $Q$, $K$, and $V$ between the different attention heads. The forthcoming math parallelizes over the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, k, v each of shape (batch, num_heads, seq_length, embed_dim)\n",
    "q = q.view(batch, seq_length, num_heads, embed_dim).permute(0, 2, 1, 3)\n",
    "k = k.view(batch, seq_length, num_heads, embed_dim).permute(0, 2, 1, 3)\n",
    "v = v.view(batch, seq_length, num_heads, embed_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "print(\"V shape:\", list(v.size()))\n",
    "print(\"V:\")\n",
    "print(v.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the dot product score:\n",
    "$$\n",
    "\\frac{QK^T}{\\sqrt d},\n",
    "$$\n",
    "which is an $L \\times L$ matrix.\n",
    "\n",
    "In particular, note that the matrix has the form:\n",
    "\\begin{align*}\n",
    "QK^T\n",
    "&= \\begin{bmatrix} q_1^T \\\\ \\vdots \\\\ q_L^T \\end{bmatrix} \\begin{bmatrix} k_1 & \\cdots & k_L \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "q_1^T k_1 & \\cdots & q_1^T k_L \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_L^T k_1 & \\cdots & q_L^T k_L\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} q_1^T K^T \\\\ \\vdots \\\\ q_L^T K^T \\end{bmatrix} .\n",
    "\\end{align*}\n",
    "Essentially, each row $i$ of the score matrix corresponds to some query $q_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.matmul(q, k.transpose(2, 3)) / np.sqrt(embed_dim)\n",
    "print(\"Score shape:\", list(score.size()))  # (batch, num_heads, seq_length, seq_length)\n",
    "print(\"Score:\")\n",
    "print(score.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the attention layer causal, we mask the score tensor so that the $(i, j)$-entry is $-\\infty$ if $i < j$. This is accomplished using the lower-triangular matrix:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 & 1 & \\cdots & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & \\cdots & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & \\cdots & 1 & 1 & 1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~(torch.tril(torch.ones(seq_length, seq_length)).bool())\n",
    "print(\"Mask shape:\", list(mask.size()))\n",
    "print(\"Mask:\")\n",
    "print(mask.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now mask $QK^T / \\sqrt{d}$ into $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.masked_fill_(mask, -torch.inf)\n",
    "print(\"Masked score:\")\n",
    "print(score.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the weight matrix $W$ through $W = \\mathrm{softmax}(S)$, where the softmax is done over the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.softmax(score, dim=3)\n",
    "print(\"Weight shape:\", list(weight.size()))\n",
    "print(\"Weight:\")\n",
    "print(weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the output $Y \\in \\mathbb{R}^{L \\times d}$ of the attention layer:\n",
    "$$\n",
    "Y = WV .\n",
    "$$\n",
    "Note that row $i$ of $Y$ corresponds to:\n",
    "$$\n",
    "\\sum_{j=1}^L w_{i,j} v_j^T = w_i^T V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = weight @ v\n",
    "print(\"Prediction shape:\", list(pred.size()))\n",
    "print(\"Prediction:\")\n",
    "print(pred.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate along the attention-head dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.permute(0, 2, 1, 3)  # (batch, seq_length, num_heads, embed_dim)\n",
    "pred = pred.reshape_as(x)\n",
    "print(\"Prediction shape:\", list(pred.size()))\n",
    "print(\"Prediction:\")\n",
    "print(pred.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the causal self-attention layer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, seq_length: int, embed_dim: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sqrt_embed_dim = np.sqrt(embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.qkv_layer = nn.Linear(embed_dim*num_heads, 3*embed_dim*num_heads, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout_prob)\n",
    "        self.resid_dropout = nn.Dropout(dropout_prob)\n",
    "        self.register_buffer(\"mask\", ~(torch.tril(torch.ones(seq_length, seq_length)).bool()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: a (batch, seq_length, embed_dim*num_heads) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, seq_length, embed_dim*num_heads) float tensor\n",
    "        \"\"\"\n",
    "        qkv = self.qkv_layer(x)                                           # (batch, seq_length, 3*embed_dim*num_heads)\n",
    "        q, k, v = torch.split(qkv, self.embed_dim*self.num_heads, dim=2)  # each tensor of shape (batch, seq_length, embed_dim*num_heads)\n",
    "\n",
    "        batch = x.size(0)\n",
    "        # q, k, v each of shape (batch, num_heads, seq_length, embed_dim)\n",
    "        q = q.view(batch, self.seq_length, self.num_heads, self.embed_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, self.seq_length, self.num_heads, self.embed_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, self.seq_length, self.num_heads, self.embed_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        score = q @ k.transpose(2, 3) / self.sqrt_embed_dim  # (batch, num_heads, seq_length, seq_length)\n",
    "\n",
    "        # causal masking\n",
    "        score.masked_fill_(self.mask, -torch.inf)\n",
    "\n",
    "        weight = torch.softmax(score, dim=3)  # (batch, num_heads, seq_length, seq_length)\n",
    "        weight = self.attn_dropout(weight)\n",
    "        pred = weight @ v                     # (batch, num_heads, seq_length, embed_dim)\n",
    "        pred = pred.permute(0, 2, 1, 3)       # (batch, seq_length, num_heads, embed_dim)\n",
    "        pred = pred.reshape_as(x)             # (batch, seq_length, embed_dim*num_heads)\n",
    "        pred = self.resid_dropout(pred)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward_with_cache(self, x: torch.Tensor, idx: int):\n",
    "        \"\"\"\n",
    "        x: a (batch, embed_dim*num_heads) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, embed_dim*num_heads) float tensor\n",
    "        \"\"\"\n",
    "        batch = x.size(0)\n",
    "        if idx == 0:\n",
    "            self._k_cache = torch.zeros(batch, self.num_heads, self.seq_length, self.embed_dim, device=x.device)\n",
    "            self._v_cache = torch.zeros_like(self._k_cache)\n",
    "\n",
    "        qkv = self.qkv_layer(x)                                           # (batch, 3*embed_dim*num_heads)\n",
    "        q, k, v = torch.split(qkv, self.embed_dim*self.num_heads, dim=1)  # each tensor of shape (batch, embed_dim*num_heads)\n",
    "\n",
    "        # q, k, v each of shape (batch, num_heads, embed_dim)\n",
    "        q = q.view(batch, self.num_heads, self.embed_dim)\n",
    "        k = k.view(batch, self.num_heads, self.embed_dim)\n",
    "        v = v.view(batch, self.num_heads, self.embed_dim)\n",
    "\n",
    "        self._k_cache[:, :, idx] = k\n",
    "        self._v_cache[:, :, idx] = v\n",
    "\n",
    "        score = (q.unsqueeze(2) @ self._k_cache.transpose(2, 3)).squeeze(2) / self.sqrt_embed_dim # (batch, num_heads, seq_length)\n",
    "\n",
    "        # causal masking\n",
    "        score[:, :, idx+1:] = -float(\"inf\")\n",
    "\n",
    "        weight = torch.softmax(score, dim=2)                       # (batch, num_heads, seq_length)\n",
    "        weight = self.attn_dropout(weight)\n",
    "        pred   = (weight.unsqueeze(2) @ self._v_cache).squeeze(2)  # (batch, num_heads, embed_dim)\n",
    "        pred   = pred.view_as(x)                                   # (batch, num_heads*embed_dim)\n",
    "        pred   = self.resid_dropout(pred)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, seq_length: int, embed_dim: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.ln_in = nn.LayerNorm(embed_dim*num_heads)\n",
    "        self.ln_out = nn.LayerNorm(embed_dim*num_heads)\n",
    "        self.attn = CausalSelfAttention(seq_length, embed_dim, num_heads, dropout_prob)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim*num_heads, embed_dim*num_heads),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*num_heads, embed_dim*num_heads),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, seq_length, embed_dim*num_heads) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, seq_length, embed_dim*num_heads) float tensor\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.ln_in(x))\n",
    "        x = x + self.mlp(self.ln_out(x))\n",
    "        return x\n",
    "\n",
    "    def forward_with_cache(self, x: torch.Tensor, idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: a (batch, embed_dim*num_heads) float tensor\n",
    "\n",
    "        Returns\n",
    "        - a (batch, embed_dim*num_heads) float tensor\n",
    "        \"\"\"\n",
    "        x = x + self.attn.forward_with_cache(self.ln_in(x), idx)\n",
    "        x = x + self.mlp(self.ln_out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(pytorch_util.MyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens: int,\n",
    "        num_pred_tokens: int,\n",
    "        seq_length: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout_prob: float = 0.,\n",
    "        concat_embed: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.concat_embed = concat_embed\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(seq_length, embed_dim, num_heads, dropout_prob) for _ in range(num_layers)])\n",
    "        if concat_embed:\n",
    "            self.token_embed_table = nn.Embedding(num_tokens, embed_dim*num_heads // 2)\n",
    "            self.pos_embed_table = nn.Embedding(seq_length, embed_dim*num_heads // 2)\n",
    "        else:\n",
    "            self.token_embed_table = nn.Embedding(num_tokens, embed_dim*num_heads)\n",
    "            self.pos_embed_table = nn.Embedding(seq_length, embed_dim*num_heads)\n",
    "        self.embed_dropout = nn.Dropout(dropout_prob)\n",
    "        self.gpt_head = nn.Linear(embed_dim*num_heads, num_pred_tokens)\n",
    "        self.print_n_params()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pos_embed_table.weight.device\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        tokens: a (batch, seq_length) long tensor of tokens with values in {0, 1, ..., T-1}, where `T-1` corresponds to <BOS>\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, seq_length, T-1)\n",
    "        \"\"\"\n",
    "        tok_embd = self.token_embed_table(tokens)    # (batch, seq_length, embed_dim*num_heads)\n",
    "        pos_embd = self.pos_embed_table.weight       # (seq_length, embed_dim*num_heads)\n",
    "        x = (torch.cat([tok_embd, pos_embd.unsqueeze(0).expand(tokens.size(0), -1, -1)], dim=2)\n",
    "             if self.concat_embed\n",
    "             else tok_embd + pos_embd)               # (batch, seq_length, embed_dim*num_heads)\n",
    "        x = self.embed_dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        logits = self.gpt_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward_with_cache(self, tokens: torch.Tensor, idx: int):\n",
    "        \"\"\"\n",
    "        tokens: a (batch,) long tensor\n",
    "        k_cache: a (batch, num_heads, seq_length, embed_dim) tensor\n",
    "        q_cache: a (batch, num_heads, seq_length, embed_dim) tensor\n",
    "        idx: an integer corresponding to which part of the cache to fill in\n",
    "\n",
    "        Returns\n",
    "        - a tensor of logits with shape (batch, T-1)\n",
    "        \"\"\"\n",
    "        tok_embd = self.token_embed_table(tokens)    # (batch, embed_dim*num_heads)\n",
    "        pos_embd = self.pos_embed_table.weight[idx]  # (embed_dim*num_heads,)\n",
    "        x = (torch.cat([tok_embd, pos_embd.unsqueeze(0).expand(tokens.size(0), -1)], dim=1)\n",
    "             if self.concat_embed\n",
    "             else tok_embd + pos_embd)               # (batch, embed_dim*num_heads)\n",
    "        x = self.embed_dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward_with_cache(x, idx)\n",
    "        logits = self.gpt_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def sample_naive(self, n_samples, prompt_token: int, record_times=False):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        - a numpy array of shape (n_samples, self.seq_length+1)\n",
    "        \"\"\"\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            samples = torch.zeros(n_samples, self.seq_length+1, device=self.device, dtype=torch.long)\n",
    "            samples[:, 0] = prompt_token\n",
    "            for i in range(self.seq_length):\n",
    "                start = time()\n",
    "                logits = self(samples)[:, i]                                         #  (n_samples, T-1)\n",
    "                probs = F.softmax(logits, 1)                                         #  (n_samples, T-1)\n",
    "                sampled_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)  #  (n_samples,)\n",
    "                end = time()\n",
    "                samples[:, i+1] = sampled_tokens\n",
    "                times.append(end-start)\n",
    "        if record_times:\n",
    "            return samples.cpu().numpy(), times\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n_samples: int, prompt_token: int, record_times: bool = False) -> np.ndarray:\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            samples = torch.zeros(n_samples, self.seq_length+1, device=self.device, dtype=torch.long)\n",
    "            samples[:, 0] = prompt_token\n",
    "            for i in range(self.seq_length):\n",
    "                start = time()\n",
    "                logits = self.forward_with_cache(samples[:, i], i)                   #  (n_samples, T-1)\n",
    "                probs = F.softmax(logits, 1)                                         #  (n_samples, T-1)\n",
    "                sampled_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)  #  (n_samples,)\n",
    "                end = time()\n",
    "                samples[:, i+1] = sampled_tokens\n",
    "                times.append(end-start)\n",
    "        if record_times:\n",
    "            return samples.cpu().numpy(), times\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a tokenizer that converts raw data into a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def tokenize(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts an input to a sequence of tokens.\n",
    "\n",
    "        x: (batch, *) array\n",
    "\n",
    "        Returns\n",
    "        - a (batch, seq_length) long array corresponding to tokens\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def detokenize(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts a sequence of tokens to a corresponding output.\n",
    "\n",
    "        x: (batch, seq_length) long array corresponding to tokens\n",
    "\n",
    "        Returns\n",
    "        - a (batch, *) array\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(arrays):\n",
    "    # taken from https://stackoverflow.com/a/11146645\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[..., i] = a\n",
    "    return arr.reshape(-1, la)\n",
    "\n",
    "\n",
    "class ImageTokenizer(Tokenizer):\n",
    "    def __init__(self, img_size, num_values):\n",
    "        \"\"\"\n",
    "        img_size: (H, W, C) tuple\n",
    "        \"\"\"\n",
    "        self.H, self.W, self.C = img_size\n",
    "        self.num_values = num_values\n",
    "        self.seq_length = self.H*self.W + 1\n",
    "        self.num_tokens = self.num_values**self.C + 1\n",
    "        self.detokenize_table = cartesian_product([np.arange(self.num_values) for _ in range(self.C)])\n",
    "\n",
    "        tokenizer_lst = [1]\n",
    "        for _ in range(self.C-1):\n",
    "            tokenizer_lst.append(tokenizer_lst[-1] * self.num_values)\n",
    "        self.tokenizer_array = np.array(list(reversed(tokenizer_lst)))\n",
    "\n",
    "    def tokenize(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        x: (batch, H, W, C) array\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        assert H == self.H\n",
    "        assert W == self.W\n",
    "        assert C == self.C\n",
    "        tokens = np.empty([B, self.seq_length], dtype=np.int64)\n",
    "        tokens[:, 0]  = self.num_tokens-1  # <BOS>\n",
    "        tokens[:, 1:] = np.einsum('c,bhwc->bhw', self.tokenizer_array, x).reshape(B, self.seq_length-1)\n",
    "        return tokens\n",
    "\n",
    "    def detokenize(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        - a (batch, H, W, C) array\n",
    "        \"\"\"\n",
    "        x = x[:, -(self.seq_length-1):]  # remove any \"prompt\" elements\n",
    "        return self.detokenize_table[x].reshape(x.shape[0], self.H, self.W, self.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the tokenizer on the black and white images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = utils.load_pickled_data(\"data/shapes.pkl\")\n",
    "print(train_data[:2].squeeze())\n",
    "print(train_data[:2].shape)\n",
    "\n",
    "print()\n",
    "tokenizer = ImageTokenizer(train_data.shape[1:], 2)\n",
    "tokens = tokenizer.tokenize(train_data[:2])\n",
    "print(tokens.squeeze())\n",
    "print(tokens.shape)\n",
    "\n",
    "print()\n",
    "x = tokenizer.detokenize(tokens)\n",
    "print(x.shape)\n",
    "print((x == train_data[:2]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: None\n",
    "\n",
    "def q3_a(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "  test_data: A (n_test, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "  image_shape: (H, W, 1), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "\n",
    "  num_values = 2\n",
    "  tokenizer = ImageTokenizer(image_shape, num_values)\n",
    "  train_data = tokenizer.tokenize(train_data)\n",
    "  test_data = tokenizer.tokenize(test_data)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  num_heads, num_layers = 4, 2\n",
    "  embed_dim = 128*num_heads\n",
    "  model = make_igpt_model(tokenizer.num_tokens, tokenizer.num_tokens-1, tokenizer.seq_length-1, embed_dim, num_heads, num_layers)\n",
    "  model.train()\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "  def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "      for test_mbatch in test_dataloader:\n",
    "        test_mbatch = test_mbatch.long().to(device)\n",
    "        logits = model(test_mbatch[:, :-1])\n",
    "        tgt = test_mbatch[:, 1:]\n",
    "        loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "        losses.append(loss.cpu().item())\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 15\n",
    "  train_steps = epochs * len(train_dataloader)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=train_steps)\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      train_mbatch = train_mbatch.long().to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch[:, :-1])\n",
    "      tgt = train_mbatch[:, 1:]\n",
    "      loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  model.eval()\n",
    "  samples = tokenizer.detokenize(model.sample(100, tokenizer.num_tokens-1))\n",
    "\n",
    "  return train_losses, test_losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_a`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: GPT(*args, concat_embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Add embeddings\")\n",
    "q3ab_save_results(1, 'a', q3_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "print(\"Add embeddings\")\n",
    "q3ab_save_results(2, 'a', q3_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: GPT(*args, concat_embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Concatenate embeddings\")\n",
    "q3ab_save_results(1, 'a', q3_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Concatenate embeddings\")\n",
    "q3ab_save_results(2, 'a', q3_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (b) iGPT on Colored Shapes and MNIST\n",
    "\n",
    "Now, implement an iGPT that models color. In order to reduce the length of token sequences, iGPT models each RGB pixel as a **single** token. This effectively reduces the context length from $H \\cdot W \\cdot C$ to just $H \\cdot W$. iGPT does this through a k-means clustering approach. Because our images only each can only take on 4 values (2 bits) per channel, we can represent each pixel with 64 values (6 bits). Convert the dataset into an image of tokens and train iGPT on the colored shapes and MNIST dataset.\n",
    "\n",
    "Checkout the iGPT paper for more details: [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "\n",
    "Training times and hyperparameter settings should be the same as part (a), except train for longer (15 epochs)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = utils.load_pickled_data(\"data/shapes_colored.pkl\")\n",
    "print(train_data[:2].squeeze())\n",
    "print(train_data[:2].shape)\n",
    "\n",
    "print()\n",
    "tokenizer = ImageTokenizer(train_data.shape[1:], 4)\n",
    "tokens = tokenizer.tokenize(train_data[:2])\n",
    "print(tokens.squeeze())\n",
    "print(tokens.shape)\n",
    "\n",
    "print()\n",
    "x = tokenizer.detokenize(tokens)\n",
    "print(x.shape)\n",
    "print((x == train_data[:2]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: None\n",
    "save_model = False\n",
    "\n",
    "def q3_b(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "\n",
    "  num_values = 4\n",
    "  tokenizer = ImageTokenizer(image_shape, num_values)\n",
    "  train_data = tokenizer.tokenize(train_data)\n",
    "  test_data = tokenizer.tokenize(test_data)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  num_heads, num_layers = 4, 2\n",
    "  embed_dim = 128*num_heads\n",
    "  model = make_igpt_model(tokenizer.num_tokens, tokenizer.num_tokens-1, tokenizer.seq_length-1, embed_dim, num_heads, num_layers)\n",
    "  model.train()\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "  def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "      for test_mbatch in test_dataloader:\n",
    "        test_mbatch = test_mbatch.long().to(device)\n",
    "        logits = model(test_mbatch[:, :-1])\n",
    "        tgt = test_mbatch[:, 1:]\n",
    "        loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "        losses.append(loss.cpu().item())\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "  \n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 15\n",
    "  train_steps = epochs * len(train_dataloader)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=train_steps)\n",
    "  for _ in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      train_mbatch = train_mbatch.long().to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch[:, :-1)\n",
    "      tgt = train_mbatch[:, 1:]\n",
    "      loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  if save_model:\n",
    "    torch.save(model.state_dict(), \"data/igpt_model.pth\")\n",
    "\n",
    "  model.eval()\n",
    "  samples = tokenizer.detokenize(model.sample(100, tokenizer.num_tokens-1))\n",
    "    \n",
    "  return train_losses, test_losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_b`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: GPT(*args, concat_embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "print(\"Add embeddings\")\n",
    "save_model = False\n",
    "q3ab_save_results(1, 'b', q3_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "print(\"Add embeddings\")\n",
    "save_model = True\n",
    "q3ab_save_results(2, 'b', q3_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_igpt_model = lambda *args: GPT(*args, concat_embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "print(\"Concatenate embeddings\")\n",
    "save_model = False\n",
    "q3ab_save_results(1, 'b', q3_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "print(\"Concatenate embeddings\")\n",
    "save_model = False\n",
    "q3ab_save_results(2, 'b', q3_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (c) K, V Caching for Improved Inference\n",
    "You may have noticed that generation from the transformer is quite slow. Part of this is just due to the autoregressive nature. However, another part is due to some computational inefficiency. At each forward pass of the model, we are performing repeat computation of the past sequence. Specifically, we can cache the key and values at the multi attention layer to more quickly predict at each step.\n",
    "\n",
    "In self-attention, a sequence is processed by generating three vectors for each element in the sequence: a Query (Q), a Key (K), and a Value (V). These vectors are then used to compute attention scores and subsequently the output of the attention layer.\n",
    "Mathematically, this can be represented as:\n",
    " * For each index $i$, compute $Q_i$, $K_i$, $V_i$ for the current element\n",
    " * Retrieve $K_{<i}$ and $V_{<i}$ from the cache (where $<i$ denotes all indices before the current one)\n",
    " * Compute the attention output using $Q_i$, $[K_{<i}, K_i]$, $[V_{<i}, V_i]$\n",
    "\n",
    "\n",
    "Next implement caching for your transformer to make inference more efficient by modifying your self attention. Use caching for inference in the future problems for faster generation! (Note caching is only used during inference). You will use the same dataset as in part B, dataset 2 of this question (colored mnist). No training is required in this section, feel free to reuse the model you trained in part B, dataset 2.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of inference, measure the time for the forward pass over the total sequence length with and without caching.\n",
    "3. 100 samples from the final trained model using the caching inference pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_c(train_data, test_data, image_shape, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "             used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# sampling steps,) numpy array of time per sampling iteration, without caching\n",
    "    - a (# sampling steps,) numpy array of time per sampling iteration, with caching\n",
    "    - a numpy array of size (100, H, C, W) of samples with values in {0, 1, 2, 3} (sample generated without caching)\n",
    "    - a numpy array of size (100, H, C, W) of samples with values in {0, 1, 2, 3} (sample generated with caching)\n",
    "    \"\"\"\n",
    "    pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "    device = pytorch_util.device\n",
    "\n",
    "    # tokenizer\n",
    "    num_values = 4\n",
    "    tokenizer = ImageTokenizer(image_shape, num_values)\n",
    "\n",
    "    # model\n",
    "    num_heads, num_layers = 4, 2\n",
    "    embed_dim = 128*num_heads\n",
    "    model = GPT(tokenizer.num_tokens, tokenizer.num_tokens-1, tokenizer.seq_length-1, embed_dim, num_heads, num_layers)\n",
    "    model.load_state_dict(torch.load(\"data/igpt_model.pth\"))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # naive sampling\n",
    "    pytorch_util.seed_rng(0)\n",
    "    samples, time_list_no_cache = model.sample_naive(100, tokenizer.num_tokens-1, True)\n",
    "    samples_no_cache = tokenizer.detokenize(samples)\n",
    "\n",
    "    # sampling with KV cache\n",
    "    pytorch_util.seed_rng(0)\n",
    "    samples, time_list_with_cache = model.sample(100, tokenizer.num_tokens-1, True)\n",
    "    samples_with_cache = tokenizer.detokenize(samples)\n",
    "\n",
    "    return time_list_no_cache, time_list_with_cache, samples_no_cache, samples_with_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_c`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "q3c_save_results(2, q3_c)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZLcaHwLNNL"
   },
   "source": [
    "# Question 4: Causal Transformer: Tokenized Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Tokenization with Vector Quanization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Image Quantization\n",
    "\n",
    "Above, we implemented iGPT, which autoregressivly predicts raw pixels. Transformers have quadratic complexity in the sequence length which prevents this naive approach from scaling well to large images.\n",
    "\n",
    "The space of natural images often contains very correlated information. This suggests we can learn a reduced representation. VQVAE is a method that does just that, learning to map images to a more compact discrete set of tokens. We will cover this method in more detail in future lectures. The only thing you need to know now is that we can learn an encoder (and corresponding decoder), which can extract a discrete representation from an image. \n",
    "\n",
    "If you are curious, checkout the VQVAE paper to learn more: https://arxiv.org/abs/1711.00937 (we will cover this in a future lecture though!)\n",
    "\n",
    "In this part, we provide a pre-trained VQVAE model, which consists of:\n",
    " * encoder to tokenize the images\n",
    " * the decoder to recover the image\n",
    " * a token vocabulary of VQVAE_MODEL.n_embeddings\n",
    "\n",
    "Below is the code for loading the VQ model. Note that VQVAE encoding process is lossy, so the decoded images will not be the exact same as the input. Some blurriness in the recovered image is to be expected. The docstrings of the relevant methods you will need for the VQVAE_MODEL are provided below for your convenience. \n",
    "\n",
    "We will use 2 colored mnist datasets in this part. The first is the same dataset used in previous parts. The second, hads a colored digit on a differently colored background. We will call these datasets Colored MNIST and Colored MNIST v2. Note that the vqvae is trained per dataset.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Use the provided encoder model to quantize the images then inspect the recovered images by applying the decoder for each of the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @property\n",
    "# def n_embeddings(self) -> int:\n",
    "#     \"\"\"The size of the token vocabulary\"\"\"\n",
    "#    \n",
    "# def quantize(self, x: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Quantize an image x.\n",
    "#\n",
    "#     Args:\n",
    "#         x (np.ndarray, dtype=int): Image to quantize. shape=(batch_size, 28, 28, 3). Values in [0, 3].\n",
    "#\n",
    "#     Returns:\n",
    "#         np.ndarray: Quantized image. shape=(batch_size, 7, 7). Values in [0, n_embeddings]\n",
    "#     \"\"\"\n",
    "#    \n",
    "# def decode(self, z_index: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Decode a quantized image.\n",
    "#\n",
    "#     Args:\n",
    "#         z_index (np.ndarray, dtype=int): Quantized image. shape=(batch_size, 7, 7). Values in [0, n_embeddings].\n",
    "#\n",
    "#     Returns:\n",
    "#         np.ndarray: Decoded image. shape=(batch_size, 28, 28, 3). Values in [0, 3].\n",
    "#     \"\"\"\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4_a(images, vqvae):\n",
    "  \"\"\"\n",
    "  images: (B, H, W, C), the images to pass through the encoder and decoder of the vqvae\n",
    "  vqvae: a vqvae model, trained on the relevant dataset\n",
    "\n",
    "  Returns\n",
    "  - a numpy array of size (B, H, W, C) of the decoded image\n",
    "  \"\"\"\n",
    "  print(\"Token vocabulary size:\", vqvae.n_embeddings)\n",
    "  encoded_images = vqvae.quantize(images)\n",
    "  decoded_images = vqvae.decode(encoded_images)\n",
    "  return decoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4a_save_results(1, q4_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4a_save_results(2, q4_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WsEzhx4Uua"
   },
   "source": [
    "## Part (b) Autoregressive Transformer on Colored Shapes and MNIST with Vector Quantization\n",
    "\n",
    "We can use the VQVAE to tokenize an image dataset. This will result in a much smaller sequence length than the approach we tried in Question 3(b). For this part, train a transformer on the dataset tokenized by the VQVAE.\n",
    "\n",
    "This is a simplified version of the approach used in VQGAN [VQGAN](https://arxiv.org/abs/2012.09841) -> Section 3.2: Learning the Composition of Images with Transformers (Again, we will cover this in more detail in a future lecture!)\n",
    "\n",
    "Update the following hyperparameters:\n",
    "* layers: 4 (we can train a bigger transformer now since less memory is used per input!)\n",
    "* 30 epochs\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4_b(train_data, test_data, image_shape, dset_id, vqvae):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "  vqvae: a vqvae model, trained on dataset dset_id\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, C, W) of samples with values in {0, 1, 2, 3}\n",
    "  \"\"\"\n",
    "  pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "  device = pytorch_util.device\n",
    "  pytorch_util.seed_rng(0)\n",
    "\n",
    "  encoded_train_data = np.expand_dims(vqvae.quantize(train_data), axis=3)\n",
    "  encoded_test_data  = np.expand_dims(vqvae.quantize(test_data),  axis=3)\n",
    "\n",
    "  tokenizer = ImageTokenizer(encoded_train_data.shape[1:], vqvae.n_embeddings)\n",
    "  tok_train_data = tokenizer.tokenize(encoded_train_data)\n",
    "  tok_test_data = tokenizer.tokenize(encoded_test_data)\n",
    "  train_losses, test_losses = [], []\n",
    "\n",
    "  num_heads, num_layers = 4, 4\n",
    "  embed_dim = 32*num_heads\n",
    "  dropout_prob = 0.1\n",
    "  model = GPT(tokenizer.num_tokens, tokenizer.num_tokens-1, tokenizer.seq_length-1, embed_dim, num_heads, num_layers, dropout_prob=dropout_prob)\n",
    "  model.train()\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  train_dataloader = DataLoader(tok_train_data, batch_size=256, shuffle=True)\n",
    "  test_dataloader = DataLoader(tok_test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "  def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "      for test_mbatch in test_dataloader:\n",
    "        test_mbatch = test_mbatch.long().to(device)\n",
    "        logits = model(test_mbatch[:, :-1])\n",
    "        tgt = test_mbatch[:, 1:]\n",
    "        loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "        losses.append(loss.cpu().item())\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "  \n",
    "  test_losses.append(evaluate())\n",
    "  epochs = 30\n",
    "  train_steps = epochs * len(train_dataloader)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=train_steps)\n",
    "  for i in tqdm(range(epochs)):\n",
    "    # train loss\n",
    "    for train_mbatch in train_dataloader:\n",
    "      train_mbatch = train_mbatch.long().to(device)\n",
    "      optimizer.zero_grad()\n",
    "      logits = model(train_mbatch[:, :-1])\n",
    "      tgt = train_mbatch[:, 1:]\n",
    "      loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-1), tgt.ravel())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      train_losses.append(loss.cpu().item())\n",
    "\n",
    "    # test loss\n",
    "    test_losses.append(evaluate())\n",
    "\n",
    "  model.eval()\n",
    "  samples = vqvae.decode(tokenizer.detokenize(model.sample(100, tokenizer.num_tokens-1)).squeeze(3))\n",
    "    \n",
    "  return train_losses, test_losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q4_b`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "q4b_save_results(1, q4_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "It_iPXaZjlk0",
    "outputId": "3ba138b1-71c9-4f67-8413-b5ef23db8204"
   },
   "outputs": [],
   "source": [
    "q4b_save_results(2, q4_b)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZLcaHwLNNL"
   },
   "source": [
    "# Question 5: Causal Transformer: Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets consider text! You are probably already fimilar with autoregressive transformers for text, now more commonly known as Large Language Modesl (LLMs).\n",
    "We will now implement a simplified version.\n",
    "\n",
    "We will be detailing with a [small poetry dataset](https://huggingface.co/datasets/merve/poetry). See some of the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = visualize_q5_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Modeling Text\n",
    "Train a transformer on the poetry dataset.\n",
    "\n",
    "Data Preprocessing:\n",
    "* We will use a simple method to tokenize the data. We will convert each unique character into a token. (Current LLMs use more sophisticated tokenizers, most commonly, [byte-pair encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt))\n",
    "* Previously we have leveraged a **\\<bos\\>** as part of the model, just like iGPT. For text, we may not always sample a sequence that starts at the beginning. Instead, we will add the **\\<bos\\>** token to the beginning of every sequence in the dataset, and remove the **\\<bos\\>** token from the model.\n",
    "* Another problem is that the model must know when to stop sampling. This is done by appending an **\\<eos\\>**, or end of sequence token at the end of every sequence in the dataset.\n",
    "* We can now convert the sequence into subsequences of size context_length, for training!\n",
    "\n",
    "We recommend the following hyperparameters:\n",
    "* Sequence length: 128\n",
    "* 5 epochs\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. Provide **5 unconditional samples** of **128 characters** showcasing the model text generation capabilities (text samples should stop after **\\<eos\\>**. Text after **\\<eos\\>** can be removed in post processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = utils.get_data_dir(1)\n",
    "train_text, _ = load_text_data(osp.join(dir_path, \"poetry.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of text snippets:\", len(train_text))\n",
    "print(\"Number of letters in first snippet:\", len(train_text[0]))\n",
    "chars = sorted(list(set(\"\".join(train_text))))\n",
    "print(\"Number of unique characters:\", len(chars))\n",
    "print(\"Unique characters:\", chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(Tokenizer):\n",
    "    def __init__(self, chars):\n",
    "        self.num_tokens = len(chars)+3\n",
    "        self.bos = \"<BOS>\"\n",
    "        self.eos = \"<EOS>\"\n",
    "        self.eos_tok = self.num_tokens-3\n",
    "        self.bos_tok = self.num_tokens-2\n",
    "        self.ignore_tok = self.num_tokens-1\n",
    "\n",
    "        self.stoi = {s: i for i, s in enumerate(chars)}\n",
    "        self.stoi[self.eos] = self.eos_tok\n",
    "        self.stoi[self.bos] = self.bos_tok\n",
    "        self.itos = {i: s for s, i in self.stoi.items()}\n",
    "\n",
    "    def tokenize(self, list_of_strings):\n",
    "        list_of_tokens = []\n",
    "        for string in list_of_strings:\n",
    "            tokens = [self.bos_tok] + [self.stoi[s] for s in string] + [self.eos_tok]\n",
    "            list_of_tokens.append(tokens)\n",
    "        return list_of_tokens\n",
    "\n",
    "    def detokenize(self, array_of_tokens):\n",
    "        list_of_strings = []\n",
    "        for tokens in array_of_tokens:\n",
    "            chars = []\n",
    "            for token in tokens:\n",
    "                if token == self.bos_tok:\n",
    "                    continue\n",
    "                if token == self.eos_tok:\n",
    "                    break\n",
    "                chars.append(self.itos[token])\n",
    "            list_of_strings.append(''.join(chars))\n",
    "        return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextTokenizer(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer.tokenize(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text sample:\", train_text[0][:10])\n",
    "print(\"Tokenization of text sample:\", train_tokens[0][:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Represents a dataset of heterogenous-length token sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_of_tokens, seq_length):\n",
    "        self.seq_length = seq_length\n",
    "        self.list_of_tokens = [self._pad_and_tensorize(tokens) for tokens in list_of_tokens]\n",
    "        self.lookup_table = self._make_lookup_table()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lookup_table.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        string_idx, start, end = self.lookup_table[idx]\n",
    "        return self.list_of_tokens[string_idx][start:end]\n",
    "\n",
    "    def _pad_and_tensorize(self, tokens):\n",
    "        if len(tokens) < self.seq_length:\n",
    "            tokens = tokens + (self.seq_length-len(tokens)) * [tokens[-1]+2]\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    def _make_lookup_table(self):\n",
    "        \"\"\"\n",
    "        idx -> (string_idx, start, end)\n",
    "        \"\"\"\n",
    "        arrays = []\n",
    "        for string_idx, tokens in enumerate(self.list_of_tokens):\n",
    "            starts = np.arange(len(tokens)-self.seq_length+1)\n",
    "            ends = starts + self.seq_length\n",
    "            arrays.append(np.stack([np.full(len(starts), string_idx), starts, ends], axis=1))\n",
    "        return np.concatenate(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDataset(train_tokens, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_lens = np.array(list(map(len, train_text)))\n",
    "print(\"Shortest text snippet length:\", snippet_lens.min())\n",
    "text_idx = snippet_lens.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (dataset.lookup_table[:,0] == text_idx).argmax()\n",
    "print(\"Corresponding tokenization:\")\n",
    "print(dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5_a(train_text, test_text):\n",
    "    \"\"\"\n",
    "    train_text: list[str] Train text sequences.\n",
    "    test_text: list[str] Test text sequences.\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a list of 5 (str), 5 generated samples from the model.\n",
    "    \"\"\"\n",
    "    pytorch_util.set_gpu_mode(\"cuda\", GPU_ID)\n",
    "    device = pytorch_util.device\n",
    "    pytorch_util.seed_rng(0)\n",
    "\n",
    "    chars = sorted(list(set(\"\".join(train_text))))\n",
    "    tokenizer = TextTokenizer(chars)\n",
    "    train_tokens = tokenizer.tokenize(train_text)\n",
    "    test_tokens = tokenizer.tokenize(test_text)\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    num_heads, num_layers = 4, 2\n",
    "    embed_dim = 16*num_heads # 32*num_heads\n",
    "    seq_length = 128\n",
    "    dropout_prob = 0.1\n",
    "    model = GPT(tokenizer.num_tokens, tokenizer.num_tokens-2, seq_length-1, embed_dim, num_heads, num_layers, dropout_prob=dropout_prob)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    train_dataset = TokenDataset(train_tokens, seq_length)\n",
    "    test_dataset = TokenDataset(test_tokens, seq_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    def evaluate():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for test_mbatch in test_dataloader:\n",
    "                test_mbatch = test_mbatch.long().to(device)\n",
    "                logits = model(test_mbatch[:, :-1])\n",
    "                tgt = test_mbatch[:, 1:]\n",
    "                loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-2), tgt.ravel(), ignore_index=tokenizer.ignore_tok)\n",
    "                losses.append(loss.cpu().item())\n",
    "        model.train()\n",
    "        return np.mean(losses)\n",
    "\n",
    "    test_losses.append(evaluate())\n",
    "    epochs = 20 # 10\n",
    "    train_steps = epochs * len(train_dataloader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=train_steps)\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        # train loss\n",
    "        for train_mbatch in train_dataloader:\n",
    "            train_mbatch = train_mbatch.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(train_mbatch[:, :-1])\n",
    "            tgt = train_mbatch[:, 1:]\n",
    "            loss = F.cross_entropy(logits.reshape(-1, tokenizer.num_tokens-2), tgt.ravel(), ignore_index=tokenizer.ignore_tok)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.cpu().item())\n",
    "\n",
    "        # test loss\n",
    "        test_losses.append(evaluate())\n",
    "\n",
    "    torch.save(model.state_dict(), \"data/text_gpt.pth\")\n",
    "    model.eval()\n",
    "    token_samples = model.sample(5, tokenizer.num_tokens-2)\n",
    "    text_samples = tokenizer.detokenize(token_samples)\n",
    "            \n",
    "    return train_losses, test_losses, text_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q5_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "q5a_save_results(q5_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZLcaHwLNNL"
   },
   "source": [
    "# Question 6: Causal Transformer: Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been dealing only with autoregressive generation of a single modality. Now we will train a model that operates on multiple modalities!\n",
    "\n",
    "We will use the text labeled colored MNIST dataset, which has a text description of the MNIST image. Run the cell below to visualize the data along with the text annotation. This is the Colored MNIST v2 dataset, which also comes with these text labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_q6_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Multimodal Text and Image Generation\n",
    "Implement and train an autoregressive (AR) model capable of handling both text and image data. The model should be designed to process sequences composed of concatenated text and image tokens in both orders (text followed by images and images followed by text). Additionally, the model should be capable of generating unconditional text and image samples.\n",
    "\n",
    "Data Preprocessing:\n",
    "* Text Tokens: Map each unique word in the text data to a unique token. (Note that all text descriptions contain the exact same amount of words. This simplifies text processing, as you won't have to deal with sequences of different lengths as in Question 5)\n",
    "* Image Tokens: Quantize the image data into tokens using the VQVAE tokenizer from Problem 4.\n",
    "* In this problem, we have 2 modalities. Introduce an **\\<end of text\\>** token and an **\\<end of image\\>** token. After seeing such a token, the model should switch to sampling the next modality.\n",
    "* Formulate batches as sequences of concat([**\\<end of image\\>**, text_tokens, **\\<end of text\\>**, image_tokens]) and concat([**\\<end of text\\>**, image_tokens, **\\<end of image\\>**, text_tokens]). With a 50/50 split between each ordering.\n",
    "\n",
    "Inference:\n",
    "* During inference, we cannot mix modality tokens. During sampling we can restrict the logits to only be within the relevant modality.\n",
    "* After **\\<end of image\\>**, only allow the model to sample text tokens (including **\\<end of text\\>**)\n",
    "* After **\\<end of text\\>**, only allow the model to sample image tokens (including **\\<end of image\\>**)\n",
    "* At the very start (conditioned on the **\\<bos\\>** token, only allow the model to sample one of (**\\<end of image\\>** or **\\<end of text\\>**))\n",
    "* As the model may not always correctly sample the **\\<end of image\\>** token before the image ends, you may add a rule to force the model to always sample the correct number of image tokens (49 tokens).\n",
    "\n",
    "You can use the same hyperparameters as in 4(b) (but of course, feel free to tune your model to achieve better performance)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 9 conditional samples based on provided text.\n",
    "4. 9 conditional samples based on provided images.\n",
    "5. 9 unconditional samples showcasing the model's capability in generating standalone text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q6_a(train_data, test_data, image_shape, train_text, test_text, image_test_prompt, text_test_prompt, vqvae):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "    image_shape: tuple (H, W, C) The shape of the images in the dataset, indicating height, width, and number of color channels.\n",
    "    train_text: list[str] Text data associated with each training image.\n",
    "    test_text: list[str] Text data associated with each test image.\n",
    "    image_test_prompt: (9, H, W, C) Image data used for generating conditional text samples during testing.\n",
    "    text_test_prompt: list of 9 strings Text prompts used for generating conditional image samples during testing.\n",
    "    vqvae: a vqvae model, trained on the relevant dataset\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a list of 9 (image, text), corresponding to the image conditioned samples\n",
    "    - a list of 9 (image, text), corresponding to the text conditions samples\n",
    "    - a list of 9 (image, text), corresponding to unconditional samples\n",
    "    \"\"\"\n",
    "    return train_losses, test_losses, samples_image_conditioned, samples_text_conditioned, samples_unconditioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGp2OsLKiToN"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q6_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "kW-k-59qJaKN",
    "outputId": "78b08f6a-12db-46b7-ac27-a07693a9cd09"
   },
   "outputs": [],
   "source": [
    "q6a_save_results(q6_a)\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": " Homework 1 Autoregressive Models (Solutions).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
